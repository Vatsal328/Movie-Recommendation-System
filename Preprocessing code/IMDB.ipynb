{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample URL template for IMDb movie pages\n",
    "base_url = \"https://www.imdb.com/title/tt{:0>7}\"  # Updated URL template with 7-digit IMDb ID\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Read the DataFrame from the provided CSV file containing IMDb movie data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Vatsal328/Movie-Recommendation-System/main/Dataset/links.csv\")\n",
    "\n",
    "# Create and open the CSV file in append mode to store scraped data\n",
    "with open('scraped_data_imdb_new.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "    # Write the header row if the file is empty\n",
    "    if f.tell() == 0:\n",
    "        f.write('imdbId,Ratings,Popularity,User Reviews,Critic Reviews,Metascore\\n')\n",
    "\n",
    "    # Iterate through each IMDb ID in the DataFrame for scraping data\n",
    "    for imdbId in df['imdbId'][8275:]:\n",
    "        imdbId_str = str(imdbId).zfill(7)  # Ensure IMDb ID is 7 digits\n",
    "        url = base_url.format(imdbId_str)\n",
    "        page = requests.get(url, headers=headers)\n",
    "\n",
    "        if page.status_code != 200:\n",
    "            print(f\"Error fetching data for IMDb ID {imdbId}. Status code: {page.status_code}\")\n",
    "            print(url)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        ratings = None\n",
    "        popularity = None\n",
    "        user_reviews = None\n",
    "        critic_reviews = None\n",
    "        metascore = None\n",
    "\n",
    "        # Extract ratings from the IMDb page\n",
    "        data_rating = soup.find('span', attrs={'class': 'sc-bde20123-1 cMEQkK'})\n",
    "        if data_rating:\n",
    "            rating = data_rating.text.strip()\n",
    "            ratings = rating\n",
    "\n",
    "        # Extract popularity score from the IMDb page\n",
    "        data_popularity = soup.find('div', {'data-testid': 'hero-rating-bar__popularity__score', 'class': 'sc-5f7fb5b4-1 fTREEx'})\n",
    "        if data_popularity:\n",
    "            popularity_score = data_popularity.text.strip().replace(',', '')  # Remove commas from popularity count\n",
    "            popularity = popularity_score\n",
    "\n",
    "        # Extract user reviews, critic reviews, and metascore from the IMDb page\n",
    "        reviews = soup.findAll('span', {'class': 'score'})\n",
    "        for i in range(len(reviews)):\n",
    "            if reviews[i] == 'User reviews':\n",
    "                user_reviews = reviews[i - 1]\n",
    "            elif reviews[i] == 'Critic reviews':\n",
    "                critic_reviews = reviews[i - 1]\n",
    "            elif reviews[i] == 'Metascore':\n",
    "                metascore = reviews[i - 1]\n",
    "\n",
    "        # Write scraped data to the CSV file\n",
    "        f.write(f'{imdbId},{ratings},{popularity},{user_reviews},{critic_reviews},{metascore}\\n')\n",
    "\n",
    "# Read the updated CSV file into a DataFrame for further analysis or verification\n",
    "scraped_data = pd.read_csv('scraped_data_imdb_new.csv')\n",
    "print(scraped_data.head())\n",
    "\n",
    "# Check the shape of the scraped_data DataFrame\n",
    "print(\"Shape of scraped_data:\", scraped_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling some unusal values(Refining Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Vatsal328/Movie-Recommendation-System/main/Dataset/links.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# df.head()\n",
    "url1 = \"https://raw.githubusercontent.com/Vatsal328/Movie-Recommendation-System/main/Preprocessing%20code/IMDB%20data.csv\"\n",
    "\n",
    "df1 = pd.read_csv(url1)\n",
    "\n",
    "# Function to convert 'k' or 'K' format to numerical value\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.upper()\n",
    "        if 'K' in value:\n",
    "            return int(float(value.replace('K', '')) * 1000)\n",
    "        else:\n",
    "            return int(value)\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return int(value) if not np.isnan(value) else np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'User Reviews' column\n",
    "df1['User Reviews'] = df1['User Reviews'].apply(convert_to_numeric)\n",
    "\n",
    "# Apply the function to the 'Critic Reviews' column\n",
    "df1['Critic Reviews'] = df1['Critic Reviews'].apply(convert_to_numeric)\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV file\n",
    "df1.to_csv('converted_data.csv', index=False)\n",
    "\n",
    "# Merge the data frames based on 'imdbId', adding missing rows with NaN values\n",
    "merged_df = pd.merge(df, df1, on='imdbId', how='left')\n",
    "\n",
    "# print(merged_df)\n",
    "\n",
    "merged_df_no_duplicates = merged_df.drop_duplicates(subset=['imdbId'], keep='first')\n",
    "\n",
    "print(merged_df_no_duplicates)\n",
    "\n",
    "merged_df_no_duplicates.to_csv('merged_df_no_duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle duplicate columns not needed now\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/Vatsal328/Movie-Recommendation-System/main/Preprocessing%20code/IMDB%20data.csv\"\n",
    "\n",
    "\n",
    "# df = pd.read_csv(url)\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "# # Remove duplicated columns and rename them\n",
    "# df = df.rename(columns={'movieId_x': 'movieId', 'tmdbId_x': 'tmdbId'})\n",
    "\n",
    "# # Drop movieId_y and tmdbId_y columns\n",
    "# df.drop(['movieId_y', 'tmdbId_y'], axis=1, inplace=True)\n",
    "\n",
    "# print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
